{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "difficult-french",
   "metadata": {},
   "source": [
    "### Execution on CIFAR dataset\n",
    "\n",
    "This notebook aims to reproduce results of the paper: \n",
    "\n",
    "*Detection of Adversarial Attacks by Analyzing Deep Features with Multivariate Data Algorithms*\n",
    "\n",
    "\n",
    "It allows to execute XGBoost on the hidden layers of CIFAR model, and reproduce our results.\n",
    "\n",
    "\n",
    "For each individual layer and each individual attack:\n",
    "\n",
    "- it trains XGBoost\n",
    "- it attempts detection also on the other attacks\n",
    "\n",
    "Different approaches/optimizations are tested, as described also in the paper. \n",
    "\n",
    "\n",
    "It requires the layers available at https://drive.google.com/drive/folders/1JsV45ooRlk5CpqFCPy-uR4iBB3Nbqx08?usp=sharing ,\n",
    "folder CIFAR\n",
    "\n",
    "\n",
    "We assume the CIFAR folder is located at path BASE (e.g., BASE=/home/whatever/CIFAR )\n",
    "\n",
    "\n",
    "We recommend to use the conda environment available on the github.\n",
    "<br><br>\n",
    "<br><br>\n",
    "**For brevity, we consider only the 4 best layers, 156, 105, 20, 88, and we train only on the attacks 'fgsm_0156', 'deep_10'**\n",
    "\n",
    "This can be changed easily in the second box below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-source",
   "metadata": {},
   "source": [
    "#### Configure the following items attentively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-handbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET=\"cifar\"\n",
    "BATCH_SIZE=128 \n",
    "EPOCH=30 #useful only for fastai; the model include a stopping criteria so no problem with high Epochs\n",
    "LOGFILE=\"cifar_log_0_8.csv\" #just a name, choose one\n",
    "CPU_THREADS=32 # just to limit cpu usage, can be set to any value\n",
    "RESULTS='/home/notebook/neuron/results/' #the path to the log file\n",
    "BASE='/home/notebook/neuron/layers/tmp/'  #put your path to CIFAR folder\n",
    "index_path='/home/notebook/neuron/indexes/' #path to the indexes, as available on google drive. \n",
    "                                            #These are required to properly create train and test\n",
    "    \n",
    "MODEL_SAVE='/home/notebook/neuron/models_08/' #it will save here the models it creates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-porter",
   "metadata": {},
   "source": [
    "From now on, everything should go smoothly (but training can be slow at some times)\n",
    "\n",
    "The notebook will train the model, execute prediction and log results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACK=['bim', 'carlini_l2', 'deep_10', 'fgsm_02', 'fgsm_0156']\n",
    "SUFFIX='.npy'\n",
    "MAIN_ATTACK_LIST=['deep_10', 'fgsm_0156']\n",
    "TOP_LAYERS=[156, 105, 20, 88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patent-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import fastai\n",
    "from fastai.tabular.all import *\n",
    "from pathlib import Path\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "%env OMP_NUM_THREADS=CPU_THREADS\n",
    "%env MKL_NUM_THREADS=CPU_THREADS\n",
    "\n",
    "torch.set_num_threads(CPU_THREADS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-munich",
   "metadata": {},
   "source": [
    "Functions to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data for a specific layer\n",
    "def load_train(L):\n",
    "    p = Path(BASE+\"/train/layer_\"+str(L)+\".npy\")\n",
    "    with p.open('rb') as f:\n",
    "        nptmp=np.load(f)\n",
    "    return nptmp\n",
    "\n",
    "def load_test(L):\n",
    "    p = Path(BASE+\"/test/layer_\"+str(L)+\".npy\")\n",
    "    with p.open('rb') as f:\n",
    "        nptmp=np.load(f)\n",
    "    return nptmp\n",
    "\n",
    "#prefix, layer\n",
    "def load_attack(attack, L):\n",
    "    p = Path(BASE+\"/\"+str(attack)+\"/layer_\"+str(L)+\".npy\")\n",
    "    print(p)\n",
    "    with p.open('rb') as f:\n",
    "        nptmp=np.load(f)\n",
    "    return nptmp\n",
    "\n",
    "def load_4_layers_train(a, b=None, c=None, d=None):\n",
    "    a=load_train(a)\n",
    "    if(b!=None):\n",
    "        b=load_train(b)\n",
    "    if(c!=None):\n",
    "        c=load_train(c)\n",
    "    if(d!= None):\n",
    "        d=load_train(d)\n",
    "    return a, b, c, d\n",
    "\n",
    "def load_4_layers_test(a, b=None, c=None, d=None):\n",
    "    a=load_test(a)\n",
    "    if(b!=None):\n",
    "        b=load_test(b)\n",
    "    if(c!=None):\n",
    "        c=load_test(c)\n",
    "    if(d!= None):\n",
    "        d=load_test(d)\n",
    "    return a, b, c, d\n",
    "\n",
    "def load_4_layers_attack(attack, a=None, b=None, c=None, d=None):\n",
    "    a=load_attack(attack, a)\n",
    "    if(b!=None):\n",
    "        b=load_attack(attack, b)\n",
    "    if(c!=None):\n",
    "        c=load_attack(attack, c)\n",
    "    if(d!= None):\n",
    "        d=load_attack(attack, d)\n",
    "    return a, b, c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-finish",
   "metadata": {},
   "source": [
    "Function to linearize the hidden features for a target layer and image. This allows to apply tabular data algorithms on the hidden features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod(val): \n",
    "    res = 1 \n",
    "    for ele in val: \n",
    "        res *= ele \n",
    "    return res  \n",
    "\n",
    "def linearize(numpy_linearized):\n",
    "    shape_tuple=numpy_linearized.shape[1:]\n",
    "    row_length=prod(list(shape_tuple))\n",
    "    numpy_linearized=numpy_linearized.reshape(\n",
    "        numpy_linearized.shape[0], row_length)\n",
    "    return numpy_linearized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-document",
   "metadata": {},
   "source": [
    "Prepare to log data\n",
    "\n",
    "We log:\n",
    "\n",
    "- METHOD\n",
    "- how we split  (train/test)\n",
    "- layer number(s)\n",
    "- attack for test\n",
    "- attack used for training\n",
    "- DATASET (CIFAR)\n",
    "- metrics: accuracy, tn, fp, fn, tp, \n",
    "- tpr when fpr is < 0.05, to compare with evadeML\n",
    "- tpr when fpr is < 0.015, to compare with MagNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-calcium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log_stacking_split(logfile, METHOD, split,approach, layer, attack, attack1, DATASET, accuracy, tn, fp, fn, tp, tpr1, tpr2):\n",
    "    logfile.write( \"{}, {}, {}, {}, {}, {}, {}, {:.3f}, {}, {}, {}, {}, {:.3f}, {:.3f}\\n\".format(METHOD,\n",
    "                                    approach,\n",
    "                                    split,\n",
    "                                    layer,\n",
    "                                    attack,\n",
    "                                    attack1,\n",
    "                                    DATASET,\n",
    "                                    accuracy,\n",
    "                                    tn, fp, fn, tp,\n",
    "                                    tpr1, tpr2))\n",
    "    logfile.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(RESULTS+LOGFILE, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8c29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write(\"ALG, APPROACH, SPLIT 0.8 or 0.9, LAYER, ATTACK, TRAINED ON, DATASET, ACCURACY, TN, FP, FN, TP, TPR with FPR < 0.05, TPR with FPR < 0.015 \\n\")\n",
    "f.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-duncan",
   "metadata": {},
   "source": [
    "The following creates the train and test datasets.\n",
    "\n",
    "For example, they match each normal data point to label 0, and each attack data point to label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normal(normal_x):\n",
    "    normal_y=np.empty([normal_x.shape[0], 1])\n",
    "    normal_y.fill(0)\n",
    "    return normal_x, normal_y\n",
    "\n",
    "def create_attack(attack_x):\n",
    "    attack_y=np.empty([attack_x.shape[0], 1])\n",
    "    attack_y.fill(1)\n",
    "    return attack_x, attack_y\n",
    "\n",
    "def create_dataframe(x, y):\n",
    "    LABEL_NUMBER=x.shape[1]\n",
    "    df=pd.DataFrame(np.concatenate((x, y), axis=1))\n",
    "    df[LABEL_NUMBER]=df[LABEL_NUMBER].astype(str)\n",
    "    cont_names, cat_names = cont_cat_split(pd.DataFrame(x))\n",
    "\n",
    "    return df, cont_names, cat_names, LABEL_NUMBER\n",
    "\n",
    "def create_test_set(numpy_test_x,attack_test_x, attack_test_y):\n",
    "    numpy_test_x=linearize(numpy_test_x)\n",
    "    numpy_test_y=np.empty([numpy_test_x.shape[0], 1])\n",
    "    numpy_test_y.fill(0)\n",
    "    merged_x=np.concatenate((numpy_test_x, attack_test_x), axis=0)\n",
    "    merged_y=np.concatenate((numpy_test_y, attack_test_y), axis=0)\n",
    "    df=pd.DataFrame(merged_x)\n",
    "    return df , merged_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-instrumentation",
   "metadata": {},
   "source": [
    "Now the training and evaluation can start:\n",
    "\n",
    "- take a layer\n",
    "- take an attack\n",
    "- organize a train/test set\n",
    "- do supervised train using XGBOOST\n",
    "- do predict on the test set (legitimate images + attack images)\n",
    "- do predict on all the other attacks\n",
    "\n",
    "\n",
    "To be fair, we create a test set which includes \"normal image + adversarial image\", balanced, and both of them have never been seen at training time. This is hard coded here, and cannot be parametrized. \n",
    "\n",
    "We provide this approach for a train-test split of 0.8 (default). The user can configure the test split, just modifying obvious parameters.\n",
    "\n",
    "(Note: the test set may contain additional images, which does not have a corresponding \"adversarial part\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the index of the \"indexes arrays\" (see the folder indexes) at which we have the 0.8 train/test split for the under consideration.\n",
    "#we need it for deep_10 and fgsm_0156\n",
    "TEST_SPLIT_08=[#('bim', 7873, 'bim_0.008_cifar_index_final.npy'),\n",
    "               #('carlini_l2', 7059, 'carlini_l2_cifar_index_final.npy'),\n",
    "               ('deep_10',7824, 'deep_10_cifar_index_final.npy'),\n",
    "               #('fgsm_02', 0, ''),\n",
    "               ('fgsm_0156',7059, 'fgsm_0.0156_cifar_index_final.npy')]\n",
    "\n",
    "TEST_SPLIT=[0.8]\n",
    "\n",
    "\n",
    "list_combo= [(L, y,d)\n",
    "             for L in TOP_LAYERS \n",
    "             for y in TEST_SPLIT \n",
    "             for d in MAIN_ATTACK_LIST\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_attack_set(test_split, attack1, split_value, split_index,\n",
    "                      x_attack_156,\n",
    "                      x_attack_105=None,\n",
    "                      x_attack_20=None,\n",
    "                      x_attack_88=None):\n",
    "    if(attack1==\"deep_10\"):\n",
    "        SPLIT_PLACE=test_split[2] #deep\n",
    "    elif(attack1==\"fgsm_0156\"):\n",
    "        SPLIT_PLACE=test_split[4] #fgsm_0156\n",
    "\n",
    "    indexes=np.load(index_path+SPLIT_PLACE[2])\n",
    "    index_current_attack=np.min(np.where(indexes >= split_index))\n",
    "\n",
    "    x_attack_156=x_attack_156[index_current_attack:]\n",
    "    if(x_attack_105 is not None):\n",
    "        x_attack_105=x_attack_105[index_current_attack:]\n",
    "    if(x_attack_20 is not None):\n",
    "        x_attack_20=x_attack_20[index_current_attack:]\n",
    "    if(x_attack_88 is not None):\n",
    "        x_attack_88=x_attack_88[index_current_attack:]\n",
    "    return x_attack_156, x_attack_105, x_attack_20, x_attack_88\n",
    "\n",
    "\n",
    "def define_attack_set_1(attack1, target_size, x_attack_156,\n",
    "                      x_attack_105=None,\n",
    "                      x_attack_20=None,\n",
    "                      x_attack_88=None):\n",
    "    if(x_attack_156.shape[0]> target_size):\n",
    "        index_current_attack=x_attack_156.shape[0]-target_size-1\n",
    "    else:\n",
    "        index_current_attack=0\n",
    "\n",
    "    x_attack_156=x_attack_156[index_current_attack:]\n",
    "    if(x_attack_105 is not None):\n",
    "        x_attack_105=x_attack_105[index_current_attack:]\n",
    "    if(x_attack_20 is not None):\n",
    "        x_attack_20=x_attack_20[index_current_attack:]\n",
    "    if(x_attack_88 is not None):\n",
    "        x_attack_88=x_attack_88[index_current_attack:]\n",
    "\n",
    "    return x_attack_156, x_attack_105, x_attack_20, x_attack_88\n",
    "\n",
    "\n",
    "def analyze_split(test_split, attack):\n",
    "    split=\"split:\"+str(TEST_SPLIT[0])\n",
    "    split_value=TEST_SPLIT[0]\n",
    "    #create attack train and attack test\n",
    "    if(attack==\"fgsm_0156\"):\n",
    "        SPLIT_PLACE=TEST_SPLIT_08[1] #fgsm\n",
    "    elif(attack==\"deep_10\"): #deepfool\n",
    "        SPLIT_PLACE=TEST_SPLIT_08[0] #deepfool\n",
    "#    elif(attack==\"bim\"): \n",
    "#        SPLIT_PLACE=test_split[0]\n",
    "#    elif(attack==\"carlini_l2\"): \n",
    "#        SPLIT_PLACE=test_split[1]\n",
    "#    elif(attack==\"fgsm_02\"): \n",
    "#        SPLIT_PLACE=test_split[3]\n",
    "\n",
    "    split_index_test=SPLIT_PLACE[1]\n",
    "    return split, split_value, split_index_test\n",
    "\n",
    "def split_test_normal(x_test_156, test_elements, x_test_105=None, x_test_20=None, x_test_88=None ):\n",
    "    size=x_test_156.shape[0]\n",
    "    x_test_156=x_test_156[size-(test_elements+1):]\n",
    "    if(x_test_105 is not None):\n",
    "        x_test_105=x_test_105[size-(test_elements+1):]\n",
    "    if(x_test_20 is not None):\n",
    "        x_test_20=x_test_20[size-(test_elements+1):]\n",
    "    if(x_test_88 is not None):\n",
    "        x_test_88=x_test_88[size-(test_elements+1):]\n",
    "    return x_test_156,x_test_105, x_test_20, x_test_88\n",
    "\n",
    "\n",
    "def attack_split(attack_array, split_value):\n",
    "    x_attack_train=attack_array[0:round(split_value*attack_array.shape[0])]\n",
    "    x_attack_test=attack_array[round(split_value*attack_array.shape[0]):]\n",
    "    return x_attack_train, x_attack_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these just computes metrics\n",
    "def compute_metrics_and_log(y_test, y_attack_test, final_preds, final_preds_proba, approach, METHOD, split, layer):\n",
    "    accuracy=accuracy_score(np.vstack((y_test, y_attack_test)), final_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(np.vstack((y_test, y_attack_test)), final_preds, labels=[0,1]).ravel()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(np.vstack((y_test, y_attack_test)), final_preds_proba[:, 1].ravel())\n",
    "    tpr_05=tpr[np.argmax(fpr[fpr<=0.05])]\n",
    "    tpr_016=tpr[np.argmax(fpr[fpr<=0.016])]\n",
    "    write_log_stacking_split(f, METHOD, split, approach, layer, attack, \"trained on normal + {}\".format(attack), \n",
    "                       DATASET, accuracy, tn, fp, fn, tp, tpr_05, tpr_016)\n",
    "\n",
    "def compute_metrics_and_log_attacks(y_test, y_attack, final_preds, final_preds_proba, approach, METHOD, split, layer):\n",
    "    accuracy=accuracy_score(y_attack, final_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_attack, final_preds, labels=[0,1]).ravel()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(np.vstack((y_test, y_attack)),\n",
    "                                                 final_preds_proba[:, 1].ravel())\n",
    "    #tpr_05=tpr[np.argmax(fpr[fpr<=0.05])] not good when multiple values with the same number\n",
    "    #tpr_016=tpr[np.argmax(fpr[fpr<=0.016])] not good when multiple values with the same number\n",
    "    #replaced with:\n",
    "    \n",
    "    tpr_05=tpr[np.argwhere(fpr == np.amax(fpr[fpr<=0.05])).flatten()][-1]\n",
    "    tpr_016=tpr[np.argwhere(fpr == np.amax(fpr[fpr<=0.016])).flatten()][-1]\n",
    "    write_log_stacking_split(f, METHOD,split, approach, layer, attack1, attack, \n",
    "                       DATASET, accuracy, tn, fp, fn, tp, tpr_05, tpr_016)\n",
    "    \n",
    "    \n",
    "    \n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # method I: plt\n",
    "    plt.title(\"layer: {}, trained on {}, tested on {}\".format(layer, attack, attack1))\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-plumbing",
   "metadata": {},
   "source": [
    "Just a plain approach: on each layer, it applies XGBoost and compute metrics.\n",
    "\n",
    "It saves the models that are later used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, test_split, attack in list_combo:\n",
    "    split, split_value, split_index=analyze_split(test_split, attack)\n",
    "    #load train 156, 105, 20, 88\n",
    "    x_train= load_train(layer)\n",
    "    y_train = np.zeros((x_train.shape[0],1))\n",
    "\n",
    "    #load attacks of the three layers\n",
    "    x_attack=load_attack(attack, layer)\n",
    "    print(\"{} shape of attacks in train + test  {}\".format(attack, x_attack.shape))\n",
    "    \n",
    "    #split also attacks, to maintain the same test & attack portion of data\n",
    "    x_attack_train, x_attack_test= attack_split(x_attack, split_value)\n",
    "    print(\"{} shape in the test set is {} after split of {}\".format(attack, x_attack_test.shape,split))\n",
    "\n",
    "    #these are OK like this for all layers\n",
    "    y_attack_train=np.ones((x_attack_train.shape[0],1))\n",
    "    y_attack_test=np.ones((x_attack_test.shape[0],1))\n",
    "\n",
    "    \n",
    "    #load test normal 156, 105, 20, 88\n",
    "    x_test= load_test(layer)\n",
    "    x_test, useless1, useless2, useless3=split_test_normal(x_test,x_attack_test.shape[0])#take the last x_attack_test.shape[0] element from test set\n",
    "\n",
    "    #y_test is built the same for all three layers\n",
    "    y_test = np.zeros((x_test.shape[0],1))\n",
    "   \n",
    "    \n",
    "    #initialize classifier\n",
    "    xgbC=xgb.XGBClassifier(nthread=CPU_THREADS,\n",
    "                           use_label_encoder=False,\n",
    "                           objective= 'binary:logistic',\n",
    "                           eval_metric='logloss')\n",
    "    #fit    \n",
    "    print(\"now training\")\n",
    "    #xgbC.fit(linearize(np.vstack((x_train,x_attack_train))), np.vstack((y_train, y_attack_train)))\n",
    "    #xgbC.save_model(MODEL_SAVE+str(attack)+'_'+str(layer)+'.model')\n",
    "    xgbC.load_model(MODEL_SAVE+str(attack)+'_'+str(layer)+'.model')\n",
    "\n",
    "    #predict on the test set and analysis of results\n",
    "    print(\"now running predictions\")\n",
    "    final_preds=xgbC.predict(linearize(np.vstack((x_test, x_attack_test))))\n",
    "    final_preds_proba=xgbC.predict_proba(linearize(np.vstack((x_test, x_attack_test))))\n",
    "\n",
    "    #compute metrics\n",
    "    \n",
    "    compute_metrics_and_log(y_test, y_attack_test, \n",
    "                            final_preds, final_preds_proba, \n",
    "                            approach= \"just one layer\", METHOD=\"XGBoost only\", split=split, layer=layer)\n",
    "   \n",
    "    #now repeat prediction on each attack\n",
    "    #we need to recover the test split applied for the other attack\n",
    "    for attack1 in ATTACK:\n",
    "        #load attacks \n",
    "        x_attack =load_attack(attack1, layer)\n",
    "\n",
    "        x_attack, useless1, useless2, useless3=define_attack_set_1(attack1, x_attack_test.shape[0], x_attack)        \n",
    "        \n",
    "        y_attack_test=np.ones((x_attack.shape[0],1))\n",
    "        print(\"{} shape after split {}\".format(attack1, x_attack.shape))\n",
    "        final_preds=xgbC.predict(linearize(x_attack))\n",
    "        final_preds_proba=xgbC.predict_proba(linearize(np.vstack((x_test,x_attack))))\n",
    "        compute_metrics_and_log_attacks(y_test, y_attack_test, final_preds,\n",
    "                                                             final_preds_proba,\n",
    "                                                             approach= \"just one layer\",\n",
    "                                                             METHOD=\"XGBoost only\",\n",
    "                                                             split=split, layer=layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-present",
   "metadata": {},
   "source": [
    "We now use average pooling to reduce the size of layer 105. This can slightly improve performance if we reduce its size.\n",
    "\n",
    "We try the following reduction parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=[ \n",
    "    #C, W, H, stride1, stride2, stride3\n",
    "#    (2, 3, 3, 2, 2, 2),\n",
    "#    (2, 3, 3, 2, 2, 1),\n",
    "#    (1, 6, 6, 2, 2, 1),\n",
    "#    (2, 6, 6, 3, 3, 1),\n",
    "#    (3, 6, 6, 2, 2, 2),\n",
    "    (2, 4, 4, 2, 2, 1)\n",
    "    ]\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, C, H, W, stride1, stride2, stride3):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.premodel_layer_1 = nn.Sequential(\n",
    "            nn.AvgPool2d((H, W), stride=stride1)\n",
    "        )\n",
    "        self.premodel_layer_2 = nn.Sequential(\n",
    "            nn.AvgPool2d((1, C), stride=(1,stride3))\n",
    "        )\n",
    "        \n",
    "    def forward(self, out):\n",
    "        out = self.premodel_layer_1(torch.tensor(out))\n",
    "        out=torch.transpose(out, 1, 3)\n",
    "        out = self.premodel_layer_2(out)\n",
    "        out=out.detach().numpy()\n",
    "        out=np.squeeze(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_fgsm_deep(attack):\n",
    "    if(attack==\"fgsm_0156\"):\n",
    "        C, H, W, stride1, stride2, stride3= 2, 4, 4, 2, 2, 1\n",
    "        pre_model=NeuralNetwork(C, W, H, stride1, stride2, stride3)\n",
    "    #linearize deep_10\n",
    "    elif(attack==\"deep_10\"):\n",
    "        C, H, W, stride1, stride2, stride3= 3, 6, 6, 2, 2, 2\n",
    "        pre_model=NeuralNetwork(C, W, H, stride1, stride2, stride3)\n",
    "    return C, H, W, stride1, stride2, stride3, pre_model\n",
    "\n",
    "def reduce_pre_model(x_train_105, x_test_105, x_attack_105, pre_model):\n",
    "    x_train_105=pre_model(x_train_105)\n",
    "    x_test_105=pre_model(x_test_105)\n",
    "    x_attack_105=pre_model(x_attack_105)\n",
    "    return x_train_105, x_test_105, x_attack_105"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-florence",
   "metadata": {},
   "source": [
    "The following just prints the parameters that will be tested in the next block\n",
    "\n",
    "From running the models, it will result that the following configuration should be selected for deep_10 and fgsm_0156:\n",
    "\n",
    "C, W, H, stride1, stride2, stride3:  (2, 4, 4, 2, 2, 1)\n",
    "\n",
    "This reduction brings layer 105 from (8, 8, 304) to (151, 3, 7) i.e., from 19456 features values to 3171. This reduction can improve detection on layer 105. So it will be used from now on.\n",
    "\n",
    "The corresponding models will be created with the names\n",
    "\n",
    "*deep_10_105_244221_reduced.model*\n",
    "\n",
    "*fgsm_0156_105_244221_reduced.model*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, test_split, attack in list_combo:\n",
    "    for param in parameters:\n",
    "        if(layer != 105):\n",
    "            continue\n",
    "        print(attack, analyze_split(test_split, attack)[1], param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, test_split, attack in list_combo:\n",
    "    for param in parameters:\n",
    "        if(layer != 105):\n",
    "            continue\n",
    "        split, split_value, split_index=analyze_split(test_split, attack)\n",
    "\n",
    "        #load train 105\n",
    "        x_train= load_train(layer)\n",
    "        y_train = np.zeros((x_train.shape[0],1))\n",
    "\n",
    "        #load attacks of the three layers\n",
    "        x_attack=load_attack(attack, layer)\n",
    "        print(\"{} shape of attacks in train + test  {}\".format(attack, x_attack.shape))\n",
    "    \n",
    "        #split also attacks, to maintain the same test & attack portion of data\n",
    "        x_attack_train, x_attack_test= attack_split(x_attack, split_value)\n",
    "        print(\"{} shape in the test set is {} after split of {}\".format(attack, x_attack_test.shape,split))\n",
    "\n",
    "        #these are OK like this for all layers\n",
    "        y_attack_train=np.ones((x_attack_train.shape[0],1))\n",
    "        y_attack_test=np.ones((x_attack_test.shape[0],1))\n",
    "        \n",
    "        #load test normal 105\n",
    "        x_test= load_test(layer)\n",
    "        x_test, useless1, useless2, useless3=split_test_normal(x_test,x_attack_test.shape[0])\n",
    "\n",
    "        #y_test is built the same for all three layers\n",
    "        y_test = np.zeros((x_test.shape[0],1))\n",
    "\n",
    "\n",
    "\n",
    "        #initialize classifier\n",
    "        xgbC=xgb.XGBClassifier(nthread=CPU_THREADS,\n",
    "                           use_label_encoder=False,\n",
    "                           objective= 'binary:logistic',\n",
    "                           eval_metric='logloss')\n",
    "\n",
    "        C, W, H, stride1, stride2, stride3=param\n",
    "        pre_model=NeuralNetwork(C, W, H, stride1, stride2, stride3)\n",
    "        x_train=pre_model(x_train)\n",
    "        x_attack_train=pre_model(x_attack_train)\n",
    "        x_test=pre_model(x_test)\n",
    "        x_attack_test=pre_model(x_attack_test)\n",
    "        print(\"shape of hidden features after avgpool is {}\".format(x_train.shape[1:]))\n",
    "    \n",
    "        #fit    \n",
    "        print(\"now training\")\n",
    "        xgbC.fit(linearize(np.vstack((x_train,x_attack_train))), np.vstack((y_train, y_attack_train)))\n",
    "        xgbC.save_model(MODEL_SAVE+str(attack)+\"_\"+str(layer)+\"_\"+\n",
    "                        str(C)+str(W)+str(H)+str(stride1)+str(stride2)+str(stride3)+\"_reduced.model\")\n",
    "\n",
    "        #predict on the test set and analysis of results\n",
    "        print(\"now running predictions\")\n",
    "        final_preds=xgbC.predict(linearize(np.vstack((x_test, x_attack_test))))\n",
    "        final_preds_proba=xgbC.predict_proba(linearize(np.vstack((x_test, x_attack_test))))\n",
    "\n",
    "        #compute metrics\n",
    "        compute_metrics_and_log(y_test, y_attack_test, \n",
    "                            final_preds, final_preds_proba, \n",
    "                            approach= \"layer 105 with avg pool\", METHOD=\"XGBoost only\", split=split, layer=layer)\n",
    "   \n",
    "        #now repeat prediction on each attack\n",
    "        #we need to recover the test split applied for the other attack\n",
    "        for attack1 in ATTACK:\n",
    "            #load attacks \n",
    "            \n",
    "            x_attack =load_attack(attack1, layer)\n",
    "            x_attack, useless1, useless2, useless3=define_attack_set_1(attack1, x_attack_test.shape[0],x_attack)        \n",
    "        \n",
    "            y_attack_test=np.ones((x_attack.shape[0],1))\n",
    "            print(\"{} shape after split {}\".format(attack1, x_attack.shape))\n",
    "            x_attack=pre_model(x_attack)\n",
    "            final_preds=xgbC.predict(linearize(x_attack))\n",
    "            final_preds_proba=xgbC.predict_proba(linearize(np.vstack((x_test,x_attack))))\n",
    "            compute_metrics_and_log_attacks(y_test, y_attack_test, final_preds, final_preds_proba,\n",
    "                                        approach= \"layer 105 with avg pool\", METHOD=\"XGBoost only\", \n",
    "                                        split=split, layer=layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-metropolitan",
   "metadata": {},
   "source": [
    "We now apply the same reasoning and use average pooling to reduce the size of layer 20. This can slightly improve performance if we reduce its size.\n",
    "\n",
    "\n",
    "Layer 20 has shape: (32, 32, 12) i.e., 12288 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, test_split, attack in list_combo:\n",
    "    for param in parameters:\n",
    "        if(layer != 20):\n",
    "            continue\n",
    "\n",
    "        split, split_value, split_index=analyze_split(test_split, attack)\n",
    "\n",
    "        #load train 20\n",
    "        x_train= load_train(layer)\n",
    "        y_train = np.zeros((x_train.shape[0],1))\n",
    "\n",
    "        #load attacks of the three layers\n",
    "        x_attack=load_attack(attack, layer)\n",
    "        print(\"{} shape of attacks in train + test  {}\".format(attack, x_attack.shape))\n",
    "    \n",
    "        #split also attacks, to maintain the same test & attack portion of data\n",
    "        x_attack_train, x_attack_test= attack_split(x_attack, split_value)\n",
    "        print(\"{} shape in the test set is {} after split of {}\".format(attack, x_attack_test.shape,split))\n",
    "\n",
    "        #these are OK like this for all layers\n",
    "        y_attack_train=np.ones((x_attack_train.shape[0],1))\n",
    "        y_attack_test=np.ones((x_attack_test.shape[0],1))\n",
    "        \n",
    "        #load test normal 20\n",
    "        x_test= load_test(layer)\n",
    "        x_test, useless1, useless2, useless3=split_test_normal(x_test,x_attack_test.shape[0])\n",
    "\n",
    "        #y_test is built the same for all three layers\n",
    "        y_test = np.zeros((x_test.shape[0],1))\n",
    "\n",
    "\n",
    "        #initialize classifier\n",
    "        xgbC=xgb.XGBClassifier(nthread=CPU_THREADS,\n",
    "                           use_label_encoder=False,\n",
    "                           objective= 'binary:logistic',\n",
    "                           eval_metric='logloss')\n",
    "\n",
    "        C, W, H, stride1, stride2, stride3=param\n",
    "        pre_model=NeuralNetwork(C, W, H, stride1, stride2, stride3)\n",
    "        x_train=pre_model(x_train)\n",
    "        x_attack_train=pre_model(x_attack_train)\n",
    "        x_test=pre_model(x_test)\n",
    "        x_attack_test=pre_model(x_attack_test)\n",
    "        print(\"shape of hidden features after avgpool is {}\".format(x_train.shape[1:]))\n",
    "    \n",
    "        #fit    \n",
    "        print(\"now training\")\n",
    "        xgbC.fit(linearize(np.vstack((x_train,x_attack_train))), np.vstack((y_train, y_attack_train)))\n",
    "        xgbC.save_model(MODEL_SAVE+str(attack)+\"_\"+str(layer)+\"_\"+\n",
    "                        str(C)+str(W)+str(H)+str(stride1)+str(stride2)+str(stride3)+\"_reduced.model\")\n",
    "\n",
    "        #predict on the test set and analysis of results\n",
    "        print(\"now running predictions\")\n",
    "        final_preds=xgbC.predict(linearize(np.vstack((x_test, x_attack_test))))\n",
    "        final_preds_proba=xgbC.predict_proba(linearize(np.vstack((x_test, x_attack_test))))\n",
    "\n",
    "        #compute metrics\n",
    "        compute_metrics_and_log(y_test, y_attack_test, \n",
    "                            final_preds, final_preds_proba, \n",
    "                            approach= \"layer 20 with avg pool\", METHOD=\"XGBoost only\", split=split, layer=layer)\n",
    "   \n",
    "        #now repeat prediction on each attack\n",
    "        #we need to recover the test split applied for the other attack\n",
    "        for attack1 in ATTACK:\n",
    "            #load attacks \n",
    "            x_attack =load_attack(attack1, layer)\n",
    "            x_attack, useless1, useless2, useless3=define_attack_set_1(attack1, x_attack_test.shape[0],x_attack)        \n",
    "        \n",
    "            y_attack_test=np.ones((x_attack.shape[0],1))\n",
    "            print(\"{} shape after split {}\".format(attack1, x_attack.shape))\n",
    "            x_attack=pre_model(x_attack)\n",
    "            final_preds=xgbC.predict(linearize(x_attack))\n",
    "            final_preds_proba=xgbC.predict_proba(linearize(np.vstack((x_test,x_attack))))\n",
    "            compute_metrics_and_log_attacks(y_test, y_attack_test, final_preds, final_preds_proba,\n",
    "                                        approach= \"layer 20 with avg pool\", METHOD=\"XGBoost only\", \n",
    "                                        split=split, layer=layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-judgment",
   "metadata": {},
   "source": [
    "We now use average pooling to reduce the size of layer 88. This can slightly improve performance if we reduce its size.\n",
    "\n",
    "Layer 88 has shape: (16, 16, 12), that is 3072 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, test_split, attack in list_combo:\n",
    "    for param in parameters:\n",
    "        if(layer != 88):\n",
    "            continue\n",
    "\n",
    "        split, split_value, split_index=analyze_split(test_split, attack)\n",
    "\n",
    "        #load train 88\n",
    "        x_train= load_train(layer)\n",
    "        y_train = np.zeros((x_train.shape[0],1))\n",
    "\n",
    "        #load attacks \n",
    "        x_attack=load_attack(attack, layer)\n",
    "        print(\"{} shape of attacks in train + test  {}\".format(attack, x_attack.shape))\n",
    "    \n",
    "        #split also attacks, to maintain the same test & attack portion of data\n",
    "        x_attack_train, x_attack_test= attack_split(x_attack, split_value)\n",
    "        print(\"{} shape in the test set is {} after split of {}\".format(attack, x_attack_test.shape,split))\n",
    "\n",
    "        #these are OK like this for all layers\n",
    "        y_attack_train=np.ones((x_attack_train.shape[0],1))\n",
    "        y_attack_test=np.ones((x_attack_test.shape[0],1))\n",
    "        \n",
    "        #load test normal 88\n",
    "        x_test= load_test(layer)\n",
    "        x_test, useless1, useless2, useless3=split_test_normal(x_test,x_attack_test.shape[0])\n",
    "\n",
    "        #y_test is built the same for all three layers\n",
    "        y_test = np.zeros((x_test.shape[0],1))\n",
    "\n",
    "        #initialize classifier\n",
    "        xgbC=xgb.XGBClassifier(nthread=CPU_THREADS,\n",
    "                           use_label_encoder=False,\n",
    "                           objective= 'binary:logistic',\n",
    "                           eval_metric='logloss')\n",
    "\n",
    "        C, W, H, stride1, stride2, stride3=param\n",
    "        pre_model=NeuralNetwork(C, W, H, stride1, stride2, stride3)\n",
    "        x_train=pre_model(x_train)\n",
    "        x_attack_train=pre_model(x_attack_train)\n",
    "        x_test=pre_model(x_test)\n",
    "        x_attack_test=pre_model(x_attack_test)\n",
    "        print(\"shape of hidden features after avgpool is {}\".format(x_train.shape[1:]))\n",
    "    \n",
    "        #fit    \n",
    "        print(\"now training\")\n",
    "        xgbC.fit(linearize(np.vstack((x_train,x_attack_train))), np.vstack((y_train, y_attack_train)))\n",
    "        xgbC.save_model(MODEL_SAVE+str(attack)+\"_\"+str(layer)+\"_\"+\n",
    "                        str(C)+str(W)+str(H)+str(stride1)+str(stride2)+str(stride3)+\"_reduced.model\")\n",
    "\n",
    "        #predict on the test set and analysis of results\n",
    "        print(\"now running predictions\")\n",
    "        final_preds=xgbC.predict(linearize(np.vstack((x_test, x_attack_test))))\n",
    "        final_preds_proba=xgbC.predict_proba(linearize(np.vstack((x_test, x_attack_test))))\n",
    "\n",
    "        #compute metrics\n",
    "        compute_metrics_and_log(y_test, y_attack_test, \n",
    "                            final_preds, final_preds_proba, \n",
    "                            approach= \"layer 88 with avg pool\", METHOD=\"XGBoost only\", split=split, layer=layer)\n",
    "   \n",
    "        #now repeat prediction on each attack\n",
    "        #we need to recover the test split applied for the other attack\n",
    "        for attack1 in ATTACK:\n",
    "            #load attacks \n",
    "            x_attack =load_attack(attack1, layer)\n",
    "            x_attack, useless1, useless2, useless3=define_attack_set_1(attack1, x_attack_test.shape[0],x_attack)        \n",
    "        \n",
    "            y_attack_test=np.ones((x_attack.shape[0],1))\n",
    "            print(\"{} shape after split {}\".format(attack1, x_attack.shape))\n",
    "            x_attack=pre_model(x_attack)\n",
    "            final_preds=xgbC.predict(linearize(x_attack))\n",
    "            final_preds_proba=xgbC.predict_proba(linearize(np.vstack((x_test,x_attack))))\n",
    "            compute_metrics_and_log_attacks(y_test, y_attack_test, final_preds, final_preds_proba,\n",
    "                                        approach= \"layer 88 with avg pool\", METHOD=\"XGBoost only\", \n",
    "                                        split=split, layer=layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-parameter",
   "metadata": {},
   "source": [
    "Analysing the log, we can  observe that the preferred avgpool for 105 is the configuration:\n",
    "\n",
    "- *deep_10_105_244221_reduced.model*\n",
    "- *fgsm_0156_105_244221_reduced.model*\n",
    "\n",
    "For layer 20:\n",
    "\n",
    "- there is no actual improvement, so no reduced model is used\n",
    "\n",
    "For layer 88, it is: \n",
    "\n",
    "- there is no actual improvement, so no reduced model is use\n",
    "\n",
    "\n",
    "**We will use the reduced models we identfied, from now on, instead than the original model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-newport",
   "metadata": {},
   "source": [
    "Now we try 2 layers in stacking: 156 and 105, the best 2 layers we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_classifiers():\n",
    "    a=xgb.XGBClassifier(nthread=CPU_THREADS,use_label_encoder=False,objective='binary:logistic',eval_metric='logloss')\n",
    "    b=xgb.XGBClassifier(nthread=CPU_THREADS,use_label_encoder=False,objective='binary:logistic',eval_metric='logloss')\n",
    "    c=xgb.XGBClassifier(nthread=CPU_THREADS,use_label_encoder=False,objective='binary:logistic',eval_metric='logloss')\n",
    "    d=xgb.XGBClassifier(nthread=CPU_THREADS,use_label_encoder=False,objective='binary:logistic',eval_metric='logloss')    \n",
    "    e=xgb.XGBClassifier(nthread=CPU_THREADS,use_label_encoder=False,objective='binary:logistic',eval_metric='logloss')    \n",
    "    return a, b, c, d, e\n",
    "\n",
    "list_combo= [(y,d)\n",
    "             for y in TEST_SPLIT \n",
    "             for d in MAIN_ATTACK_LIST\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_xgb(layer156,xgb_156,layer105=None, xgb_105=None,layer20=None,xgb_20=None, \n",
    "                      layer88=None, xgb_88=None):\n",
    "    meta_156=xgb_156.predict_proba(linearize(layer156))\n",
    "    meta_105=None\n",
    "    meta_20=None\n",
    "    meta_88=None #just to initialize vars\n",
    "    if(layer105 is not None):\n",
    "        meta_105=xgb_105.predict_proba(linearize(layer105))\n",
    "    if(layer20 is not None):\n",
    "        meta_20=xgb_20.predict_proba(linearize(layer20))\n",
    "    if(layer88 is not None):\n",
    "        meta_88=xgb_88.predict_proba(linearize(layer88))\n",
    "    return meta_156, meta_105, meta_20, meta_88\n",
    "\n",
    "#reduces layer 105 of C 2 W 4 H 4 stride1 2 stride2 2 stride3 1  \n",
    "def avgpool_reduction(a,b=None,c=None, d=None):\n",
    "    C, W, H, stride1, stride2, stride3=2, 4, 4, 2, 2, 1\n",
    "    pre_model=NeuralNetwork(C, W, H, stride1, stride2, stride3)\n",
    "    a=pre_model(a)\n",
    "    if(b is not None):\n",
    "        b=pre_model(b)\n",
    "    if(c is not None):\n",
    "        c=pre_model(c)\n",
    "    if(d is not None):\n",
    "        d=pre_model(d)\n",
    "    return a,b,c,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_split, attack in list_combo:\n",
    "    split, split_value, split_index=analyze_split(test_split, attack)\n",
    "    #and for all attacks\n",
    "    x_attack_156, x_attack_105, useless1, useless2=load_4_layers_attack(attack, 156, 105)\n",
    "    #split attacks as usual, to maintain the same test & attack portion of data\n",
    "    x_attack_train_156, x_attack_test_156= attack_split(x_attack_156, split_value)\n",
    "    x_attack_train_105, x_attack_test_105= attack_split(x_attack_105, split_value)\n",
    "    #these are OK like this for all layers\n",
    "    y_attack_train=np.ones((x_attack_train_156.shape[0],1))\n",
    "    y_attack_test=np.ones((x_attack_test_156.shape[0],1))\n",
    "    \n",
    "    #load train, create y_train\n",
    "    x_train_156,x_train_105,useless1, useless2= load_4_layers_train(156, 105)\n",
    "    y_train = np.zeros((x_train_156.shape[0],1))\n",
    "    #same for test\n",
    "    x_test_156,x_test_105, useless1, useless2= load_4_layers_test(156, 105)\n",
    "    x_test_156,x_test_105, useless1, useless2=split_test_normal(x_test_156, x_attack_test_156.shape[0], x_test_105)\n",
    "    y_test = np.zeros((x_test_156.shape[0],1))\n",
    "\n",
    "    #initialize all classifiers (base and stacker)\n",
    "    xgb_TOT, xgb_156, xgb_105, useless1, useless2=initialize_classifiers()\n",
    "    \n",
    "    xgb_156.load_model(MODEL_SAVE+str(attack)+\"_\"+str(156)+\".model\")\n",
    "    xgb_105.load_model(MODEL_SAVE+str(attack)+\"_105_244221_reduced.model\")\n",
    "    \n",
    "    x_train_105,x_attack_train_105,x_attack_test_105, x_test_105=avgpool_reduction(x_train_105,x_attack_train_105,\n",
    "                                                                                   x_attack_test_105, x_test_105)\n",
    "    #prediction made by base classifiers (create meta-data for stacking)\n",
    "    meta_156, meta_105, useless1, useless2 =predict_proba_xgb(np.vstack((x_train_156,x_attack_train_156)),xgb_156,\n",
    "                                                              np.vstack((x_train_105,x_attack_train_105)),xgb_105)\n",
    "\n",
    "    #fit the meta-learner    \n",
    "    print(\"training meta classifier\")\n",
    "    xgb_TOT.fit(np.column_stack((meta_156, meta_105)), np.vstack((y_train, y_attack_train)))\n",
    "    xgb_TOT.save_model(MODEL_SAVE+\"xgboost_stacker_156_105.model\")\n",
    "\n",
    "    #predict on the test set and analysis of results\n",
    "    print(\"now running predictions\")\n",
    "    meta_156_test_normal, meta_105_test_normal, useless1, useless2=predict_proba_xgb(x_test_156,xgb_156,\n",
    "                                                                                      x_test_105,xgb_105)\n",
    "    \n",
    "    meta_156_test_attack,meta_105_test_attack,useless1, useless2=predict_proba_xgb(x_attack_test_156,xgb_156,\n",
    "                                                                                x_attack_test_105,xgb_105)\n",
    "    \n",
    "    meta_test_normal=np.column_stack((meta_156_test_normal, meta_105_test_normal))\n",
    "    meta_test_attack=np.column_stack((meta_156_test_attack,meta_105_test_attack))\n",
    "    \n",
    "    final_preds=xgb_TOT.predict(np.vstack((meta_test_normal, meta_test_attack)))\n",
    "\n",
    "    final_preds_proba=xgb_TOT.predict_proba(np.vstack((meta_test_normal, meta_test_attack)))\n",
    "\n",
    "    #compute metrics\n",
    "    compute_metrics_and_log(y_test, y_attack_test, \n",
    "                final_preds, final_preds_proba, \n",
    "                approach= \"stacking of 156 and 105\", METHOD=\"XGBoost only\", split=split, layer=None)\n",
    "\n",
    "\n",
    "    #now repeat prediction on each attack\n",
    "    #we would need to recover the test split from the other one\n",
    "    for attack1 in ATTACK:\n",
    "        #load attacks \n",
    "        x_attack_156, x_attack_105, useless1, useless2 =load_4_layers_attack(attack1, 156, 105)\n",
    "        x_attack_156, x_attack_105, useless1, useless2=define_attack_set_1(attack1,\n",
    "                                                                         x_attack_test_156.shape[0],\n",
    "                                                                         x_attack_156,\n",
    "                                                                         x_attack_105)\n",
    "\n",
    "        x_attack_105,useless1,useless2, useless3=avgpool_reduction(x_attack_105)\n",
    "        \n",
    "        y_attack_test=np.ones((x_attack_156.shape[0],1))\n",
    "        meta_156,meta_105,useless1, useless2=predict_proba_xgb(x_attack_156,xgb_156,x_attack_105, xgb_105)\n",
    "    \n",
    "        meta_test_attack=np.column_stack((meta_156, meta_105))\n",
    "        final_preds=xgb_TOT.predict(meta_test_attack)\n",
    "        final_preds_proba=xgb_TOT.predict_proba(np.vstack((meta_test_normal,meta_test_attack)))\n",
    "\n",
    "        compute_metrics_and_log_attacks(y_test, y_attack_test, \n",
    "                    final_preds, final_preds_proba, \n",
    "                    approach= \"stacking of 156 and 105\", METHOD=\"XGBoost only\", split=split, layer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-photograph",
   "metadata": {},
   "source": [
    "Now we try 3 layers in stacking: 156, 105, 20, the best 3 layers we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_split, attack in list_combo:\n",
    "\n",
    "    split, split_value, split_index=analyze_split(test_split, attack)\n",
    "    #load train, create y_train\n",
    "    x_train_156,x_train_105,x_train_20, useless2= load_4_layers_train(156, 105, 20)\n",
    "    y_train = np.zeros((x_train_156.shape[0],1))\n",
    "    x_attack_156, x_attack_105, x_attack_20, useless2=load_4_layers_attack(attack, 156, 105, 20)\n",
    "    #split attacks as usual, to maintain the same test & attack portion of data\n",
    "    x_attack_train_156, x_attack_test_156= attack_split(x_attack_156, split_value)\n",
    "    x_attack_train_105, x_attack_test_105= attack_split(x_attack_105, split_value)\n",
    "    x_attack_train_20, x_attack_test_20= attack_split(x_attack_20, split_value)\n",
    "    #these are OK like this for all layers\n",
    "    y_attack_train=np.ones((x_attack_train_156.shape[0],1))\n",
    "    y_attack_test=np.ones((x_attack_test_156.shape[0],1))\n",
    "    \n",
    "    \n",
    "    #same for test\n",
    "    x_test_156,x_test_105, x_test_20, useless2= load_4_layers_test(156, 105, 20)\n",
    "    x_test_156,x_test_105, x_test_20, useless2=split_test_normal(x_test_156, x_attack_test_156.shape[0], x_test_105, x_test_20)\n",
    "    y_test = np.zeros((x_test_156.shape[0],1))\n",
    "    #and for all attacks\n",
    "\n",
    "    #initialize all classifiers (base and stacker)\n",
    "    xgb_TOT, xgb_156, xgb_105, xgb_20, useless2=initialize_classifiers()\n",
    "    \n",
    "    xgb_156.load_model(MODEL_SAVE+str(attack)+\"_\"+str(156)+\".model\")\n",
    "    xgb_105.load_model(MODEL_SAVE+str(attack)+\"_105_244221_reduced.model\")\n",
    "    xgb_20.load_model(MODEL_SAVE+str(attack)+\"_\"+str(20)+\".model\")\n",
    "    \n",
    "    x_train_105,x_attack_train_105,x_attack_test_105, x_test_105=avgpool_reduction(x_train_105,x_attack_train_105,x_attack_test_105, x_test_105)\n",
    "    \n",
    "    #prediction made by base classifiers (create meta-data for stacking)\n",
    "    meta_156, meta_105, meta_20, useless2 =predict_proba_xgb(np.vstack((x_train_156,x_attack_train_156)),xgb_156,\n",
    "                                                              np.vstack((x_train_105,x_attack_train_105)),xgb_105,\n",
    "                                                              np.vstack((x_train_20,x_attack_train_20)),xgb_20,\n",
    "                                                            )\n",
    "\n",
    "    #fit the meta-learner    \n",
    "    print(\"training meta classifier\")\n",
    "    xgb_TOT.fit(np.column_stack((meta_156, meta_105, meta_20)), np.vstack((y_train, y_attack_train)))\n",
    "    xgb_TOT.save_model(MODEL_SAVE+\"xgboost_stacker_156_105_20.model\")\n",
    "\n",
    "    #predict on the test set and analysis of results\n",
    "    print(\"now running predictions\")\n",
    "    meta_156_test_normal, meta_105_test_normal, meta_20_test_normal, useless2=predict_proba_xgb(x_test_156,xgb_156,\n",
    "                                                                                      x_test_105,xgb_105,\n",
    "                                                                                      x_test_20,xgb_20)\n",
    "    \n",
    "    meta_156_test_attack,meta_105_test_attack,meta_20_test_attack, useless2=predict_proba_xgb(x_attack_test_156,xgb_156,\n",
    "                                                                                x_attack_test_105,xgb_105,\n",
    "                                                                                x_attack_test_20,xgb_20)\n",
    "    \n",
    "    meta_test_normal=np.column_stack((meta_156_test_normal, meta_105_test_normal,meta_20_test_normal))\n",
    "    meta_test_attack=np.column_stack((meta_156_test_attack,meta_105_test_attack,meta_20_test_attack))\n",
    "    \n",
    "    final_preds=xgb_TOT.predict(np.vstack((meta_test_normal, meta_test_attack)))\n",
    "\n",
    "    final_preds_proba=xgb_TOT.predict_proba(np.vstack((meta_test_normal, meta_test_attack)))\n",
    "\n",
    "    #compute metrics\n",
    "    compute_metrics_and_log(y_test, y_attack_test, \n",
    "                final_preds, final_preds_proba, \n",
    "                approach= \"stacking of 156-105-20\", METHOD=\"XGBoost only\", split=split, layer=None)\n",
    "\n",
    "\n",
    "    #now repeat prediction on each attack\n",
    "    #we would need to recover the test split from the other one\n",
    "    for attack1 in ATTACK:\n",
    "        #load attacks of the three layers\n",
    "        x_attack_156, x_attack_105, x_attack_20, useless2 =load_4_layers_attack(attack1, 156, 105, 20)\n",
    "\n",
    "        x_attack_156, x_attack_105, x_attack_20, useless2=define_attack_set_1(attack1,\n",
    "                                                                            x_attack_test_156.shape[0],\n",
    "                                                                            x_attack_156,\n",
    "                                                                            x_attack_105,\n",
    "                                                                            x_attack_20)\n",
    "        x_attack_105, useless1,useless2, useless3=avgpool_reduction(x_attack_105)\n",
    "        \n",
    "        y_attack_test=np.ones((x_attack_156.shape[0],1))\n",
    "\n",
    "        meta_156,meta_105,meta_20, useless2=predict_proba_xgb(x_attack_156,xgb_156,x_attack_105, xgb_105,x_attack_20, xgb_20)\n",
    "    \n",
    "        meta_test_attack=np.column_stack((meta_156, meta_105, meta_20))\n",
    "        final_preds=xgb_TOT.predict(meta_test_attack)\n",
    "        final_preds_proba=xgb_TOT.predict_proba(np.vstack((meta_test_normal,meta_test_attack)))\n",
    "\n",
    "        compute_metrics_and_log_attacks(y_test, y_attack_test, \n",
    "                    final_preds, final_preds_proba, \n",
    "                    approach= \"stacking of 156-105-20\", METHOD=\"XGBoost only\", split=split, layer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-connecticut",
   "metadata": {},
   "source": [
    "Now we try 4 layers in stacking: 156, 105, 20, 88\n",
    "\n",
    "We should notice a very slight improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_split, attack in list_combo:\n",
    "    split, split_value, split_index=analyze_split(test_split, attack)\n",
    "    #load train, create y_train\n",
    "    x_train_156,x_train_105,x_train_20, x_train_88= load_4_layers_train(156, 105, 20, 88)\n",
    "    y_train = np.zeros((x_train_156.shape[0],1))\n",
    "#and for all attacks\n",
    "    x_attack_156, x_attack_105, x_attack_20,x_attack_88=load_4_layers_attack(attack, 156, 105, 20, 88)\n",
    "    #split attacks as usual, to maintain the same test & attack portion of data\n",
    "    x_attack_train_156, x_attack_test_156= attack_split(x_attack_156, split_value)\n",
    "    x_attack_train_105, x_attack_test_105= attack_split(x_attack_105, split_value)\n",
    "    x_attack_train_20, x_attack_test_20= attack_split(x_attack_20, split_value)\n",
    "    x_attack_train_88, x_attack_test_88= attack_split(x_attack_88, split_value)\n",
    "    #these are OK like this for all layers\n",
    "    y_attack_train=np.ones((x_attack_train_156.shape[0],1))\n",
    "    y_attack_test=np.ones((x_attack_test_156.shape[0],1))\n",
    "\n",
    "    \n",
    "    #same for test\n",
    "    x_test_156,x_test_105, x_test_20, x_test_88= load_4_layers_test(156, 105, 20,88)\n",
    "    x_test_156,x_test_105, x_test_20, x_test_88=split_test_normal(x_test_156, \n",
    "                                                                  x_attack_test_156.shape[0],\n",
    "                                                                  x_test_105, x_test_20, \n",
    "                                                                  x_test_88)\n",
    "    y_test = np.zeros((x_test_156.shape[0],1))\n",
    "    \n",
    "    #initialize all classifiers (base and stacker)\n",
    "    xgb_TOT, xgb_156, xgb_105, xgb_20,xgb_88=initialize_classifiers()\n",
    "    \n",
    "    xgb_156.load_model(MODEL_SAVE+str(attack)+\"_\"+str(156)+\".model\")\n",
    "    xgb_105.load_model(MODEL_SAVE+str(attack)+\"_105_244221_reduced.model\")\n",
    "    xgb_20.load_model(MODEL_SAVE+str(attack)+\"_\"+str(20)+\".model\")\n",
    "    xgb_88.load_model(MODEL_SAVE+str(attack)+\"_\"+str(88)+\".model\")\n",
    "\n",
    "    #avgpool on 105 as usual\n",
    "    x_train_105,x_attack_train_105,x_attack_test_105, x_test_105=avgpool_reduction(x_train_105,x_attack_train_105,x_attack_test_105, x_test_105)\n",
    "    #prediction made by base classifiers (create meta-data for stacking)\n",
    "    meta_156, meta_105, meta_20, meta_88 =predict_proba_xgb(np.vstack((x_train_156,x_attack_train_156)),xgb_156,\n",
    "                                                              np.vstack((x_train_105,x_attack_train_105)),xgb_105,\n",
    "                                                              np.vstack((x_train_20,x_attack_train_20)),xgb_20,\n",
    "                                                              np.vstack((x_train_88,x_attack_train_88)),xgb_88,\n",
    "                                                           )\n",
    "\n",
    "    #fit the meta-learner    \n",
    "    print(\"training meta classifier\")\n",
    "    xgb_TOT.fit(np.column_stack((meta_156, meta_105, meta_20,meta_88)), np.vstack((y_train, y_attack_train)))\n",
    "    xgb_TOT.save_model(MODEL_SAVE+\"xgboost_stacker_156_105_20_88.model\")\n",
    "\n",
    "    #predict on the test set and analysis of results\n",
    "    print(\"now running predictions\")\n",
    "    meta_156_test_normal, meta_105_test_normal, meta_20_test_normal, meta_88_test_normal=predict_proba_xgb(x_test_156,xgb_156,\n",
    "                                                                                      x_test_105,xgb_105,\n",
    "                                                                                      x_test_20,xgb_20,\n",
    "                                                                                      x_test_88,xgb_88)\n",
    "    \n",
    "    meta_156_test_attack,meta_105_test_attack,meta_20_test_attack, meta_88_test_attack=predict_proba_xgb(x_attack_test_156,xgb_156,\n",
    "                                                                                x_attack_test_105,xgb_105,\n",
    "                                                                                x_attack_test_20,xgb_20,\n",
    "                                                                                x_attack_test_88,xgb_88)\n",
    "    \n",
    "    meta_test_normal=np.column_stack((meta_156_test_normal, meta_105_test_normal,meta_20_test_normal,meta_88_test_normal ))\n",
    "    meta_test_attack=np.column_stack((meta_156_test_attack,meta_105_test_attack,meta_20_test_attack,meta_88_test_attack))\n",
    "    \n",
    "    final_preds=xgb_TOT.predict(np.vstack((meta_test_normal, meta_test_attack)))\n",
    "\n",
    "    final_preds_proba=xgb_TOT.predict_proba(np.vstack((meta_test_normal, meta_test_attack)))\n",
    "\n",
    "    #compute metrics\n",
    "    compute_metrics_and_log(y_test, y_attack_test, \n",
    "                final_preds, final_preds_proba, \n",
    "                approach= \"stacking of 156-105-20-88\", METHOD=\"XGBoost only\", split=split, layer=None)\n",
    "\n",
    "    #now repeat prediction on each attack\n",
    "    #we would need to recover the test split from the other one\n",
    "    for attack1 in ATTACK:\n",
    "        #load attacks of the three layers\n",
    "        x_attack_156, x_attack_105, x_attack_20, x_attack_88 =load_4_layers_attack(attack1, 156, 105, 20,88)\n",
    "\n",
    "        x_attack_156, x_attack_105, x_attack_20, x_attack_88=define_attack_set_1(attack1,\n",
    "                                                                               x_attack_test_156.shape[0],\n",
    "                                                                               x_attack_156, x_attack_105, x_attack_20,x_attack_88)\n",
    "        \n",
    "        x_attack_105, useless1,useless2, useless3=avgpool_reduction(x_attack_105)\n",
    "\n",
    "        y_attack_test=np.ones((x_attack_156.shape[0],1))\n",
    "        meta_156,meta_105,meta_20,meta_88=predict_proba_xgb(x_attack_156,xgb_156,x_attack_105, xgb_105,x_attack_20, xgb_20,x_attack_88, xgb_88)\n",
    "    \n",
    "        meta_test_attack=np.column_stack((meta_156, meta_105, meta_20, meta_88))\n",
    "        final_preds=xgb_TOT.predict(meta_test_attack)\n",
    "        final_preds_proba=xgb_TOT.predict_proba(np.vstack((meta_test_normal,meta_test_attack)))\n",
    "\n",
    "        compute_metrics_and_log_attacks(y_test, y_attack_test, \n",
    "                    final_preds, final_preds_proba, \n",
    "                    approach= \"stacking of 156-105-20-88\", METHOD=\"XGBoost only\", split=split, layer=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-lindsay",
   "metadata": {},
   "source": [
    "We compare results with XGBoost using another method for tabular data. We select fastai, because it has some nice preprocessing activities. \n",
    "\n",
    "First, we compute fastai on each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFastai_one_layer(x_train_156,x_attack_train_156,y_train, y_attack_train):\n",
    "\n",
    "    df,cont_names, cat_names, label =create_dataframe(linearize(np.vstack((x_train_156,x_attack_train_156))),\n",
    "                                                      np.vstack((y_train, y_attack_train)))\n",
    "    split = RandomSplitter(valid_pct=VAL_SPLIT, seed=42)(range_of(df))    \n",
    "    to = TabularPandas(df,procs=[Categorify, FillMissing, Normalize],\n",
    "                       cat_names = cat_names,\n",
    "                       cont_names = cont_names,\n",
    "                       y_names=label,\n",
    "                       splits=split,\n",
    "                       reduce_memory=True)\n",
    "\n",
    "    dls_156 = to.dataloaders(bs=BATCH_SIZE)#, device=torch.device('cuda'))\n",
    "    fastai_156=tabular_learner(dls_156, metrics=accuracy)\n",
    "    fastai_156.fit_one_cycle(EPOCH, cbs=[ShowGraphCallback(),\n",
    "                                         EarlyStoppingCallback(monitor='valid_loss', min_delta=0.001, patience=3)])\n",
    "    \n",
    "    return fastai_156, dls_156\n",
    "\n",
    "\n",
    "def predict_proba_fastai(x_train, fastai_156, dls_156):\n",
    "    df1=pd.DataFrame(linearize(x_train))\n",
    "    dl1 = dls_156.test_dl(df1, bs=BATCH_SIZE)#, device=torch.device('cuda')) # apply transforms\n",
    "    meta_156_fastai,  _ = fastai_156.get_preds(dl=dl1) # get prediction\n",
    "    meta_156_fastai=meta_156_fastai.numpy()\n",
    "    return meta_156_fastai\n",
    "\n",
    "\n",
    "def predict_fastai(x_train, fastai_156, dls_156):\n",
    "    df1=pd.DataFrame(linearize(x_train))\n",
    "    dl1 = dls_156.test_dl(df1, bs=BATCH_SIZE)#, device=torch.device('cuda')) # apply transforms\n",
    "    meta_156_fastai,  _ = fastai_156.get_preds(dl=dl1) # get prediction\n",
    "    meta_156_fastai=meta_156_fastai.numpy()\n",
    "    return meta_156_fastai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_combo= [(L, y,d)\n",
    "             for L in TOP_LAYERS \n",
    "             for y in TEST_SPLIT \n",
    "             for d in MAIN_ATTACK_LIST\n",
    "            ]\n",
    "\n",
    "VAL_SPLIT=0.2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-qualification",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for layer, test_split, attack in list_combo:\n",
    "    %reset_selective -f to, dls, learn, split\n",
    "    import fastai\n",
    "    from fastai.tabular.all import *\n",
    "    split, split_value, split_index=analyze_split(test_split, attack)\n",
    "\n",
    "    #load train 156, 105, 20, 88\n",
    "    x_train= load_train(layer)\n",
    "    y_train = np.zeros((x_train.shape[0],1))\n",
    "\n",
    "    #load test normal 156, 105, 20, 88\n",
    "    x_test= load_test(layer)\n",
    "    x_test, useless1, useless2, useless3=split_test_normal(x_test,split_index)\n",
    "\n",
    "    #y_test is built the same for all three layers\n",
    "    y_test = np.zeros((x_test.shape[0],1))\n",
    "\n",
    "    #load attacks of the layers\n",
    "    x_attack=load_attack(attack, layer)\n",
    "    print(\"{} shape of attacks in train + test  {}\".format(attack, x_attack.shape))\n",
    "    \n",
    "    #split also attacks, to maintain the same test & attack portion of data\n",
    "    x_attack_train, x_attack_test= attack_split(x_attack, split_value)\n",
    "    print(\"{} shape in the test set is {} after split of {}\".format(attack, x_attack_test.shape,split))\n",
    "\n",
    "    #these are OK like this for all layers\n",
    "    y_attack_train=np.ones((x_attack_train.shape[0],1))\n",
    "    y_attack_test=np.ones((x_attack_test.shape[0],1))\n",
    "\n",
    "    #avgpool on 105\n",
    "    if(layer==105):\n",
    "        x_train,x_attack_train,x_attack_test,x_test=avgpool_reduction(x_train,x_attack_train,x_attack_test,x_test)\n",
    "\n",
    "    #train fastai and save model\n",
    "    fastai_model, dls_model=trainFastai_one_layer(x_train,x_attack_train,y_train, y_attack_train)\n",
    "    fastai_model.save(MODEL_SAVE+\"fastai_\"+str(attack)+\"_\"+str(layer))\n",
    "    with open(MODEL_SAVE+\"dls_fastai_\"+str(attack)+\"_\"+str(layer), 'wb') as pickle_file:\n",
    "        pickle.dump(dls_model, pickle_file, protocol=4.0)\n",
    "\n",
    "\n",
    "    #predict on the test set and analysis of results\n",
    "    print(\"now running predictions\")\n",
    "    final_preds_proba=predict_proba_fastai(np.vstack((x_test, x_attack_test)), fastai_model, dls_model)\n",
    "    final_preds = np.argmax(final_preds_proba, axis=1)\n",
    "\n",
    "    #compute metrics\n",
    "    compute_metrics_and_log(y_test, y_attack_test, \n",
    "                            final_preds, final_preds_proba, \n",
    "                            approach= \"just one layer\", METHOD=\"FastAI only\", split=split, layer=layer)\n",
    "   \n",
    "    #now repeat prediction on each attack\n",
    "    #we need to recover the test split applied for the other attack\n",
    "    for attack1 in ATTACK:\n",
    "        #load attacks of the layer\n",
    "        x_attack =load_attack(attack1, layer)\n",
    "        if(layer==105):\n",
    "            x_attack, useless1, useless2, useless3=avgpool_reduction(x_attack)\n",
    "\n",
    "        x_attack, useless1, useless2, useless3=define_attack_set_1(test_split, attack1, split_value, split_index,x_attack)        \n",
    "        \n",
    "        y_attack_test=np.ones((x_attack.shape[0],1))\n",
    "        print(\"{} shape after split {}\".format(attack1, x_attack.shape))\n",
    "        final_preds_proba=predict_proba_fastai(np.vstack((x_test,x_attack)),fastai_model, dls_model)\n",
    "        final_preds=predict_proba_fastai(x_attack,fastai_model, dls_model)\n",
    "        final_preds = np.argmax(final_preds, axis=1)\n",
    "        \n",
    "        compute_metrics_and_log_attacks(y_test, y_attack_test, final_preds, final_preds_proba,\n",
    "                                        approach= \"just one layer\", METHOD=\"FastAI only\", split=split, layer=layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_combo= [(y,d)\n",
    "             for y in TEST_SPLIT \n",
    "             for d in MAIN_ATTACK_LIST\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-adrian",
   "metadata": {},
   "source": [
    "Compute the time it takes to XGBoost to perform the prediction with 4 stacking layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for test_split, attack in list_combo:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    x_test_156,x_test_105, x_test_20, x_test_88= load_4_layers_test(156, 105, 20,88)\n",
    "    y_test = np.zeros((x_test_156.shape[0],1))\n",
    "    #and all attacks\n",
    "    x_attack_156, x_attack_105, x_attack_20,x_attack_88=load_4_layers_attack(attack, 156, 105, 20, 88)\n",
    "    y_attack_test=np.ones((x_attack_156.shape[0],1))\n",
    "\n",
    "    #initialize all classifiers (base and stacker)\n",
    "    xgb_TOT, xgb_156, xgb_105, xgb_20,xgb_88=initialize_classifiers()\n",
    "    \n",
    "    xgb_156.load_model(MODEL_SAVE+str(attack)+\"_\"+str(156)+\".model\")\n",
    "    xgb_105.load_model(MODEL_SAVE+str(attack)+\"_105_244221_reduced.model\")\n",
    "    xgb_20.load_model(MODEL_SAVE+str(attack)+\"_\"+str(20)+\".model\")\n",
    "    xgb_88.load_model(MODEL_SAVE+str(attack)+\"_\"+str(88)+\".model\")\n",
    "    print(\"loading meta classifier\")\n",
    "    xgb_TOT.load_model(MODEL_SAVE+\"xgboost_stacker_156_105_20_88.model\")\n",
    "\n",
    "    #prepare data\n",
    "    x_test_105=np.vstack((x_test_105, x_attack_105))\n",
    "    x_test_88=np.vstack((x_test_88, x_attack_88))\n",
    "    x_test_20=np.vstack((x_test_20, x_attack_20))\n",
    "    x_test_156=np.vstack((x_test_156, x_attack_156))\n",
    "    \n",
    "    print(\"computing performance to predict on {} images\".format(x_test_105.shape[0]))\n",
    "    print(\"for attack: {} \".format(attack))\n",
    "    #predict on the test set and analysis of results\n",
    "    x_test_105,a,b,c=avgpool_reduction(x_test_105)\n",
    "    meta_156=xgb_156.predict_proba(linearize(x_test_156))\n",
    "    meta_105=xgb_105.predict_proba(linearize(x_test_105))\n",
    "    meta_20=xgb_20.predict_proba(linearize(x_test_20))\n",
    "    meta_88=xgb_88.predict_proba(linearize(x_test_88))\n",
    "\n",
    "    xgb_TOT.predict(np.column_stack((meta_156, meta_105, meta_20, meta_88)))\n",
    "    finish_time=time.time()\n",
    "    \n",
    "    print(\"--- %s seconds ---\" % (finish_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-breed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
